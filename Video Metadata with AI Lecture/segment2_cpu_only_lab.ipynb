{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3595fc1d",
   "metadata": {},
   "source": [
    "\n",
    "# Segment 2 — AI & ML Fundamentals for Video (CPU-only Lab)\n",
    "\n",
    "This Colab notebook contains **CPU-friendly exercises** using **Hugging Face** models that map to Segment 2 of your course:\n",
    "- Embeddings & Semantic Search\n",
    "- Unsupervised Learning (Clustering & PCA)\n",
    "- Named Entity Recognition (NER)\n",
    "- Vision Classification (Image tagging)\n",
    "- Whisper-based ASR (very short clips on CPU)\n",
    "- Metrics: Precision/Recall/F1\n",
    "- Transfer Learning (feature extractor → tiny classifier)\n",
    "\n",
    "> **Tip:** Run each section sequentially. Keep inputs tiny for quick CPU runs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e083ec",
   "metadata": {},
   "source": [
    "## 0) Setup (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566335a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If running on Colab CPU, install deps\n",
    "!pip -q install transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip -q install datasets sentence-transformers scikit-learn numpy pandas matplotlib librosa soundfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8c6d37",
   "metadata": {},
   "source": [
    "### Imports & Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393a79c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "print(\"Torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109128e4",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Text Embeddings + Semantic Search\n",
    "\n",
    "**Goal:** Demonstrate how **embeddings** enable **semantic search** on transcripts.\n",
    "- Model: `sentence-transformers/all-MiniLM-L6-v2` (small, fast on CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "docs = [\n",
    "  \"The pit crew changed all four tires in under two seconds.\",\n",
    "  \"The finance minister presented the annual budget today.\",\n",
    "  \"A red flag stopped the Formula 1 race due to debris on the track.\",\n",
    "  \"The actor received an award for best performance in a drama.\"\n",
    "]\n",
    "\n",
    "doc_emb = model.encode(docs, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "def search(query, top_k=3):\n",
    "    q_emb = model.encode([query], convert_to_tensor=True, normalize_embeddings=True)\n",
    "    scores = util.cos_sim(q_emb, doc_emb)[0]\n",
    "    topk = torch.topk(scores, k=top_k)\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    for idx, score in zip(topk.indices.tolist(), topk.values.tolist()):\n",
    "        print(f\"{score:.3f} :: {docs[idx]}\")\n",
    "\n",
    "# Try it:\n",
    "search(\"race was paused after an accident\", top_k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e7dc5e",
   "metadata": {},
   "source": [
    "\n",
    "**Discuss:** Why does semantic search find the F1 example without explicit keyword overlap?  \n",
    "Try your own query in `search(\"...\")`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22619ca",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Unsupervised Topic Clustering (Transcripts → Clusters)\n",
    "\n",
    "**Goal:** Use **k-means** on sentence embeddings to group topics without labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff06dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "sentences = [\n",
    "  \"Ferrari leads after an early overtake.\",\n",
    "  \"Budget deficit expected to narrow next year.\",\n",
    "  \"Mercedes pits Hamilton for hard tires.\",\n",
    "  \"Central bank maintains interest rates.\",\n",
    "  \"Safety car deployed after collision.\",\n",
    "  \"Inflation falls for third month in a row.\"\n",
    "]\n",
    "\n",
    "X = model.encode(sentences, normalize_embeddings=True)\n",
    "kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "for lbl in sorted(set(labels)):\n",
    "    print(f\"\\nCluster {lbl}:\")\n",
    "    for s, l in zip(sentences, labels):\n",
    "        if l == lbl:\n",
    "            print(\" -\", s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b63cc",
   "metadata": {},
   "source": [
    "\n",
    "**Try:** Change `n_clusters=3`. What themes emerge?  \n",
    "**Note:** In real pipelines, cluster IDs can be stored as **structural/semantic tags**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccbfe1",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Dimensionality Reduction for Visualization (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X2 = PCA(n_components=2, random_state=42).fit_transform(X)\n",
    "\n",
    "plt.figure()\n",
    "for lbl in sorted(set(labels)):\n",
    "    ix = labels == lbl\n",
    "    plt.scatter(X2[ix,0], X2[ix,1], label=f\"Cluster {lbl}\")\n",
    "for i, txt in enumerate(sentences):\n",
    "    plt.annotate(str(i), (X2[i,0], X2[i,1]))\n",
    "plt.legend()\n",
    "plt.title(\"Transcript sentence embeddings (PCA)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a92da99",
   "metadata": {},
   "source": [
    "\n",
    "**Discuss:** PCA vs t-SNE/UMAP; when visual patterns are informative vs misleading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec4d7c5",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Named Entity Recognition (NER) on Transcript\n",
    "- Model: `dslim/bert-base-NER` (CPU-friendly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff267ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\", device=-1)\n",
    "sample_transcript = \"\"\"Lewis Hamilton speaks with the Mercedes engineer on team radio during the Monaco Grand Prix.\n",
    "The FIA confirmed a five-second penalty for exceeding track limits.\"\"\"\n",
    "for ent in ner(sample_transcript):\n",
    "    print(ent[\"entity_group\"], ent[\"word\"], round(float(ent[\"score\"]), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161e0bd",
   "metadata": {},
   "source": [
    "\n",
    "**Extend:** Normalize entities (e.g., map “FIA” → governing body), dedupe, attach timestamps from ASR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e238d892",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Image Tagging for Frames (Vision Baseline)\n",
    "- Model: `google/vit-base-patch16-224` (or try `microsoft/resnet-50`)\n",
    "- **If Colab blocks downloads**, upload a couple of images via the Files pane and modify the code to load local paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import requests, io\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "vision_model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "# Provide two small images; replace with local file paths if needed\n",
    "image_urls = [\n",
    "  \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/cats.png\",\n",
    "  \"https://huggingface.co/datasets/hf-internal-testing/example-images/resolve/main/racing.jpg\"\n",
    "]\n",
    "\n",
    "def load_image(url):\n",
    "    content = requests.get(url, stream=True, timeout=10).content\n",
    "    return Image.open(io.BytesIO(content)).convert(\"RGB\")\n",
    "\n",
    "def classify_image(img: Image.Image):\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = vision_model(**inputs).logits\n",
    "    pred_id = logits.argmax(-1).item()\n",
    "    return vision_model.config.id2label[pred_id]\n",
    "\n",
    "for url in image_urls:\n",
    "    try:\n",
    "        img = load_image(url)\n",
    "        print(url, \"=>\", classify_image(img))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to load:\", url, e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf8b2c",
   "metadata": {},
   "source": [
    "\n",
    "**Extension:** Extract **embeddings** from the penultimate layer and cluster images (see Bonus section).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d178ec",
   "metadata": {},
   "source": [
    "\n",
    "## 6) ASR (Speech → Text) with Whisper-Tiny (CPU)\n",
    "- Model: `openai/whisper-tiny.en`\n",
    "- Keep clips **very short** for CPU. Upload a `.wav` (16kHz mono recommended).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f659b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-tiny.en\", device=-1)\n",
    "\n",
    "# TODO: Replace with your uploaded path, e.g., /content/sample.wav\n",
    "audio_path = \"/content/sample.wav\"\n",
    "try:\n",
    "    result = asr(audio_path)\n",
    "    print(result[\"text\"])\n",
    "except Exception as e:\n",
    "    print(\"Provide a valid short .wav file path in 'audio_path'. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4dab1",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Precision/Recall & F1 (Evaluation mini-lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df12c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
    "\n",
    "y_true = [1,0,1,0,1,0]  # ground truth for 6 items\n",
    "y_pred = [1,0,0,0,1,1]  # pretend model predictions\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Non-Racing\",\"Racing\"]))\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\")\n",
    "print(\"Precision:\", round(prec,3), \"Recall:\", round(rec,3), \"F1:\", round(f1,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62da99",
   "metadata": {},
   "source": [
    "\n",
    "**Discuss:** Search often optimizes **recall** (don’t miss relevant scenes). Compliance/brand-safety may require higher **precision**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda65fc",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Transfer Learning Lite (Feature Extractor → Classifier)\n",
    "- Use text embeddings from a pre-trained model as features for a small classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aad81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = [\n",
    "  \"Ferrari takes pole position at Monza\",\n",
    "  \"Central bank raises interest rates\",\n",
    "  \"Safety car deployed after crash\",\n",
    "  \"Quarterly earnings beat expectations\",\n",
    "  \"Pit stop under two seconds\",\n",
    "  \"Budget deficit narrows in 2025\"\n",
    "]\n",
    "y = [1,0,1,0,1,0]  # 1 = Racing, 0 = Finance\n",
    "\n",
    "emb = model.encode(texts, normalize_embeddings=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(emb, y, test_size=0.33, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))\n",
    "\n",
    "# Try your own sentence:\n",
    "test_q = \"Hamilton boxes for soft tyres\"\n",
    "pred = clf.predict(model.encode([test_q], normalize_embeddings=True))\n",
    "print(\"Pred (1=Racing):\", pred[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2def8",
   "metadata": {},
   "source": [
    "\n",
    "**Note:** This is *transfer learning* because we reuse a pre-trained encoder and only learn a tiny classifier head.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fbda9f",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Bonus: Unsupervised Frame Clustering (End-to-end)\n",
    "- Extract **image embeddings** from ViT’s hidden states and **cluster** them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce09d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reuse the ViT processor/model from earlier. We'll extract CLS token embeddings.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def image_embedding(img):\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = vision_model(**inputs, output_hidden_states=True)\n",
    "        cls = outputs.hidden_states[-1][:,0,:].squeeze(0).numpy()\n",
    "    return cls\n",
    "\n",
    "# Prepare a tiny image set: upload your own or reuse downloaded samples twice\n",
    "try:\n",
    "    imgs = []\n",
    "    for url in image_urls:\n",
    "        imgs.append(load_image(url))\n",
    "        imgs.append(load_image(url))\n",
    "\n",
    "    embs = np.vstack([image_embedding(im) for im in imgs])\n",
    "    kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "    labels = kmeans.fit_predict(embs)\n",
    "    print(\"Cluster distribution:\", {i: int(sum(labels==i)) for i in set(labels)})\n",
    "except Exception as e:\n",
    "    print(\"If downloads fail, upload a few images and modify the code to load local files. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ecd195",
   "metadata": {},
   "source": [
    "\n",
    "**Wrap-up:** Name clusters (e.g., “cars/sports” vs “misc”), and decide where to store `cluster_id` in your metadata schema.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
