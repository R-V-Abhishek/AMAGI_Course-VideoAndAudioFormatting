{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c88389",
   "metadata": {},
   "source": [
    "\n",
    "# Segment 3 — Core AI Components (CPU-only, v3 with PySceneDetect)\n",
    "This compact lab covers:\n",
    "1) Extract frames → 2) Image tagging → 3) Face detection → 4) **Shot detection (PySceneDetect)** → **Shot→Scene merge**  \n",
    "5) NER → 6) Summarization → 7) Simple RAG → 8) Export `metadata.json` (+ optional scenes CSV & thumbnails)\n",
    "\n",
    "> Tip: Use **very short** videos (≤ 20–30s) for CPU runtimes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28115f72",
   "metadata": {},
   "source": [
    "## 0) Setup (install once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722f888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CPU wheels & libs (run on Colab/local; safe to re-run)\n",
    "!pip -q install transformers==4.43.3 sentence-transformers==3.0.1 torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip -q install opencv-python==4.10.0.84 numpy==1.26.4 matplotlib==3.8.4 pillow==10.3.0 scikit-learn==1.5.1\n",
    "!pip -q install scenedetect==0.6.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909d21c",
   "metadata": {},
   "source": [
    "## 1) Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd70d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, io, csv, json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoImageProcessor, AutoModelForImageClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# PySceneDetect\n",
    "from scenedetect import open_video, SceneManager\n",
    "from scenedetect.detectors import ContentDetector, AdaptiveDetector\n",
    "\n",
    "np.random.seed(42); torch.manual_seed(42)\n",
    "print(\"OpenCV:\", cv2.__version__)\n",
    "print(\"Torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28149e",
   "metadata": {},
   "source": [
    "## 2) Extract frames (every N seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1385be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title Provide a short video path and sampling rate\n",
    "video_path = \"/content/sample.mp4\"  #@param {type:\"string\"}\n",
    "every_s = 2.0                       #@param {type:\"number\"}\n",
    "frames_dir = \"/content/frames_s3_v3\"   #@param {type:\"string\"}\n",
    "\n",
    "def extract_frames(video_path, every_s=2.0, out_dir=\"/content/frames_s3_v3\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise FileNotFoundError(f\"Cannot open {video_path}\")\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)\n",
    "    print(f\"FPS={fps:.2f}  Frames={total}\")\n",
    "    frames = []\n",
    "    next_t = 0.0\n",
    "    while True:\n",
    "        pos_s = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
    "        ok, frame = cap.read()\n",
    "        if not ok: break\n",
    "        if pos_s >= next_t:\n",
    "            out = os.path.join(out_dir, f\"frame_{int(pos_s):04d}.jpg\")\n",
    "            cv2.imwrite(out, frame); frames.append((pos_s, out))\n",
    "            next_t += every_s\n",
    "    cap.release()\n",
    "    print(\"Saved frames:\", len(frames))\n",
    "    return frames\n",
    "\n",
    "try:\n",
    "    frames = extract_frames(video_path, every_s, frames_dir)\n",
    "except Exception as e:\n",
    "    frames = []\n",
    "    print(\"Upload a short video and update video_path. Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390a68c9",
   "metadata": {},
   "source": [
    "## 3) Image tagging (ViT, CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a721b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "vision_model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "def classify_image(path):\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(images=img, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = vision_model(**inputs).logits\n",
    "    return vision_model.config.id2label[logits.argmax(-1).item()]\n",
    "\n",
    "frame_tags = []\n",
    "for t, p in (frames[:8] if frames else []):\n",
    "    try:\n",
    "        lbl = classify_image(p)\n",
    "        frame_tags.append({\"time_s\": round(t,2), \"path\": p, \"label\": lbl})\n",
    "        print(f\"{t:6.2f}s :: {os.path.basename(p)} :: {lbl}\")\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", p, e)\n",
    "if not frames:\n",
    "    print(\"No frames yet — run extraction above.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5741e63",
   "metadata": {},
   "source": [
    "## 4) Face detection (Haar cascade, CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f78c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "haar = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "face_cascade = cv2.CascadeClassifier(haar)\n",
    "\n",
    "def detect_faces(path):\n",
    "    img = cv2.imread(path); gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(40,40))\n",
    "    return int(len(faces))\n",
    "\n",
    "face_counts = []\n",
    "for t, p in (frames[:8] if frames else []):\n",
    "    n = detect_faces(p); face_counts.append({\"time_s\": round(t,2), \"path\": p, \"faces\": n})\n",
    "    print(f\"{t:6.2f}s :: faces={n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410f5f12",
   "metadata": {},
   "source": [
    "## 5) Shot detection (PySceneDetect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00290754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_shots_pyscenedetect(video_path, method=\"content\", threshold=27.0, min_scene_len=15):\n",
    "    video = open_video(video_path)\n",
    "    sm = SceneManager()\n",
    "    if method == \"adaptive\":\n",
    "        sm.add_detector(AdaptiveDetector(adaptive_threshold=3.0, min_scene_len=min_scene_len))\n",
    "    else:\n",
    "        sm.add_detector(ContentDetector(threshold=threshold, min_scene_len=min_scene_len))\n",
    "    sm.detect_scenes(video)\n",
    "    scene_list = sm.get_scene_list()\n",
    "    shots = [{\"start\": s.get_seconds(), \"end\": e.get_seconds(),\n",
    "              \"duration\": e.get_seconds()-s.get_seconds()} for s,e in scene_list]\n",
    "    return shots\n",
    "\n",
    "try:\n",
    "    shots = detect_shots_pyscenedetect(video_path, method=\"content\", threshold=27.0, min_scene_len=15)\n",
    "    print(\"Shots detected:\", len(shots))\n",
    "    print(shots[:5])\n",
    "except Exception as e:\n",
    "    shots = []\n",
    "    print(\"Shot detection failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481a555f",
   "metadata": {},
   "source": [
    "## 6) Merge shots → scenes (HSV hist + min duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e84b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hsv_hist_at_time(cap, t_sec, bins=32):\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, max(t_sec, 0)*1000.0)\n",
    "    ok, frame = cap.read()\n",
    "    if not ok: return None\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv],[0,1],None,[bins,bins],[0,180,0,256])\n",
    "    return cv2.normalize(hist, hist).flatten()\n",
    "\n",
    "def merge_shots_to_scenes(video_path, shots, min_scene_dur=3.0, max_hist_dist=0.20, bins=32):\n",
    "    if not shots: return []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    merged = [dict(shots[0])]\n",
    "    for nxt in shots[1:]:\n",
    "        cur = merged[-1]\n",
    "        t1 = (cur[\"start\"] + cur[\"end\"]) / 2.0\n",
    "        t2 = (nxt[\"start\"] + nxt[\"end\"]) / 2.0\n",
    "        h1 = hsv_hist_at_time(cap, t1, bins=bins)\n",
    "        h2 = hsv_hist_at_time(cap, t2, bins=bins)\n",
    "        dist = cv2.compareHist(h1, h2, cv2.HISTCMP_BHATTACHARYYA) if (h1 is not None and h2 is not None) else 1.0\n",
    "        should_merge = (cur[\"duration\"] < min_scene_dur) or (dist < max_hist_dist)\n",
    "        if should_merge:\n",
    "            cur[\"end\"] = nxt[\"end\"]\n",
    "            cur[\"duration\"] = cur[\"end\"] - cur[\"start\"]\n",
    "        else:\n",
    "            merged.append(dict(nxt))\n",
    "    cap.release()\n",
    "    return merged\n",
    "\n",
    "try:\n",
    "    scenes = merge_shots_to_scenes(video_path, shots, min_scene_dur=3.0, max_hist_dist=0.20)\n",
    "    print(\"Scenes (merged):\", len(scenes))\n",
    "    print(scenes[:5])\n",
    "except Exception as e:\n",
    "    scenes = []\n",
    "    print(\"Scene merge failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac99c63",
   "metadata": {},
   "source": [
    "## 7) (Optional) Export scenes CSV & thumbnails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc9ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_scenes_csv_and_thumbs(video_path, scenes, csv_path=\"/content/scenes_s3_v3.csv\", thumb_dir=\"/content/scene_thumbs_s3_v3\"):\n",
    "    if not scenes:\n",
    "        print(\"No scenes to export.\"); return None, None\n",
    "    os.makedirs(thumb_dir, exist_ok=True)\n",
    "    # CSV\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"scene_idx\",\"start_s\",\"end_s\",\"duration_s\"])\n",
    "        for i, sc in enumerate(scenes):\n",
    "            w.writerow([i, round(sc[\"start\"],2), round(sc[\"end\"],2), round(sc[\"duration\"],2)])\n",
    "    # Thumbnails (midpoint frame)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    thumbs = []\n",
    "    for i, sc in enumerate(scenes):\n",
    "        t_mid = (sc[\"start\"] + sc[\"end\"]) / 2.0\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, t_mid*1000.0)\n",
    "        ok, frame = cap.read()\n",
    "        if ok:\n",
    "            outp = os.path.join(thumb_dir, f\"scene_{i:03d}.jpg\")\n",
    "            cv2.imwrite(outp, frame); thumbs.append(outp)\n",
    "    cap.release()\n",
    "    print(\"Saved CSV:\", csv_path)\n",
    "    print(\"Saved thumbs:\", len(thumbs), \"->\", thumb_dir)\n",
    "    return csv_path, thumbs\n",
    "\n",
    "# Uncomment to export after scenes are computed:\n",
    "# csv_path, thumbs = export_scenes_csv_and_thumbs(video_path, scenes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e2124",
   "metadata": {},
   "source": [
    "## 8) NER & Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a14446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NER\n",
    "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\", device=-1)\n",
    "transcript_text = \"Lewis Hamilton spoke to Mercedes engineers during the Monaco Grand Prix. The FIA issued a penalty.\"\n",
    "entities = [{\"type\":e[\"entity_group\"], \"text\":e[\"word\"], \"score\":float(e[\"score\"])} for e in ner(transcript_text)]\n",
    "print(\"Entities:\", entities[:6])\n",
    "\n",
    "# Summarization\n",
    "from transformers import pipeline as hf_pipeline\n",
    "summarizer = hf_pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=-1)\n",
    "summary = summarizer(transcript_text, max_length=60, min_length=20, do_sample=False)[0][\"summary_text\"]\n",
    "print(\"Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c5ee2",
   "metadata": {},
   "source": [
    "## 9) Simple RAG (retrieve + template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "chunks = [\n",
    "    \"[00:00-00:10] Lewis Hamilton discussed tire strategy.\",\n",
    "    \"[00:10-00:20] Safety car was deployed after a collision.\",\n",
    "    \"[00:20-00:30] FIA announced stricter track limits.\"\n",
    "]\n",
    "emb = embedder.encode(chunks, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "def retrieve(query, top_k=2):\n",
    "    q = embedder.encode([query], convert_to_tensor=True, normalize_embeddings=True)\n",
    "    scores = util.cos_sim(q, emb)[0]\n",
    "    top = torch.topk(scores, k=top_k)\n",
    "    return [(chunks[i], float(s)) for i,s in zip(top.indices.tolist(), top.values.tolist())]\n",
    "\n",
    "def rag_answer(query):\n",
    "    items = retrieve(query, 2)\n",
    "    evidence = \"\\n\".join(f\"- {c}\" for c,_ in items)\n",
    "    return f\"Q: {query}\\nEvidence:\\n{evidence}\\nA: {items[0][0] if items else 'Insufficient evidence.'}\"\n",
    "\n",
    "print(rag_answer(\"Why was the safety car deployed?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49992a6b",
   "metadata": {},
   "source": [
    "## 10) Export `metadata.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411dda83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata = {\n",
    "    \"frames\": frame_tags,\n",
    "    \"faces\": face_counts,\n",
    "    \"shots\": shots,\n",
    "    \"scenes\": scenes,\n",
    "    \"transcript\": transcript_text,\n",
    "    \"entities\": entities,\n",
    "    \"summary\": summary,\n",
    "    \"rag_example\": rag_answer(\"Why was the safety car deployed?\")\n",
    "}\n",
    "out_path = \"/content/segment3_v3_pyscenedetect_metadata.json\"\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"Saved:\", out_path)\n",
    "print(json.dumps(metadata, indent=2)[:1200], \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
